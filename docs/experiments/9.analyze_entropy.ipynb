{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore bias towards entropy induced by activation function\n",
    "\n",
    "After observing the activations in the hidden layers over the course of training, we want to get a better feeling for the effect that the activation function has on the mutual information of these layers with the input. As the feed-forward mapping of the network is deterministic (up to numerical precision), the mutual information of the hidden layer with the input boils down to the entropy of the hidden layer. \n",
    "\n",
    "In the following we will create an artificial sample of data and manipulate it by applying different activation functions. We will compare the entropy of the output with the entropy of the original distribution. Furthermore, we will quantify the effect that different activation functions have on the entropy of the representation, which we call the inherent bias towards the entropy for that activation function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We uniformly draw 4096 samples from the interval $[-3, 3]$. Below we see the distribution of these values in a histogram with 50 bins. In this simulation the histogram counts serve as the \"activation pattern\" that could appear in a hidden layer of a neural network for uniformly distributed activations. Moreover we calculate the entropy of this hypothetical activation pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_samples = 4096\n",
    "uniform = np.random.uniform(-3, 3, (number_of_samples,))\n",
    "uniform_hist, _, _ = plt.hist(uniform, bins=50)\n",
    "entropy_uniform_dist = stats.entropy(uniform_hist, base=2)\n",
    "print(f'Entropy of the uniform distribution: {entropy_uniform_dist}')\n",
    "plt.xlabel(\"Magnitude of activations\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard_sigmoid(x):\n",
    "    lower_bound = -2.5\n",
    "    upper_bound = 2.5\n",
    "    linear = 0.2 * x + 0.5\n",
    "    linear[x < lower_bound] = 0\n",
    "    linear[x > upper_bound] = 1\n",
    "    return linear\n",
    "\n",
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "activation_functions = [tf.nn.sigmoid, tf.nn.tanh, tf.nn.relu, tf.nn.softsign, tf.nn.softplus, hard_sigmoid,\n",
    "                       tf.nn.selu, tf.nn.relu6, tf.nn.elu, tf.nn.leaky_relu, linear]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply activation functions to the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = {}\n",
    "for actvation_function in activation_functions:\n",
    "    try:\n",
    "        outputs[actvation_function.__name__] = actvation_function(uniform).numpy()\n",
    "    except AttributeError:\n",
    "        outputs[actvation_function.__name__] = actvation_function(uniform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=6, ncols=2, figsize=(10, 20), sharey=True)\n",
    "ax = ax.flat\n",
    "entropies = {}\n",
    "for ax_idx, (activation_function, Y) in enumerate(outputs.items()):\n",
    "    min_acitivity = Y.min()\n",
    "    max_acitivity = Y.max()\n",
    "   \n",
    "    bins = np.linspace(min_acitivity, max_acitivity, 50)\n",
    "    digitized, _ = np.histogram(Y, bins=bins)\n",
    "\n",
    "    entropies[activation_function] = stats.entropy(digitized, base=2)\n",
    "    \n",
    "    ax[ax_idx].hist(Y,  bins=50)\n",
    "    ax[ax_idx].set(title=f'{activation_function}; H = {entropies[activation_function]:.2f}', xlabel='activation')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(entropies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_entropies = sorted(entropies.items(), key=lambda kv: kv[1])\n",
    "xlabels = list(zip(*sorted_entropies))[0]\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(14, 6))\n",
    "for index, (activation_function, H) in enumerate(sorted_entropies):\n",
    "    ax[0].scatter(index, H)\n",
    "    ax[1].scatter(index, entropy_uniform_dist-H)\n",
    "\n",
    "ax[0].set_xticks(range(len(entropies.keys())))\n",
    "ax[0].set_xticklabels(xlabels, rotation=90)\n",
    "ax[0].set_ylabel('Entropy (bits)')\n",
    "\n",
    "ax[1].set_xticks(range(len(entropies.keys())))\n",
    "ax[1].set_xticklabels(xlabels, rotation=90)\n",
    "ax[1].set_ylabel('Inherent Compression (bits)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a percentage of previously sampled points\n",
    "relu_like_dist =_set_percentage_to_zero(uniform, 50)\n",
    "\n",
    "relu_like_hist, _, _ = plt.hist(relu_like_dist, bins=30)\n",
    "entropy_relu_like_dist = stats.entropy(relu_like_hist, base=2)\n",
    "print(f'Entropy of the relu like distribution: {entropy_relu_like_dist}')\n",
    "print(f'Compared to the uniform distribution, this is a decrease of entropy by {entropy_uniform_dist-entropy_relu_like_dist} bits.')\n",
    "plt.xlabel(\"Magnitude of activations\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can conclude from this simple experiment, that the `relu` nonlinearity has a strong bias on the mutual information of the representation with the input, as it imposes a lot of structure onto the activations. We hypothesize that `relu` activated hidden layers should therefore carry less information about the input as compared to a `tanh` activated layers. This effect should be immediate from the very beginning of the training. Moreover, within the terminology of the information bottleneck, this boils down to \"immediate compression\". The exact magnitude of this effect within the experiments we ran has still to be estimated. \n",
    "\n",
    "In order to visualize the effect of setting portions of the data samples to zero, we plot the the resulting entropy against the proportion of samples which are set to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropies = []\n",
    "for percentage in np.arange(0, 100, 1):\n",
    "    relu_like = _set_percentage_to_zero(uniform, percentage)\n",
    "    relu_like_hist, _ = np.histogram(relu_like, bins=30)\n",
    "    entropy_relu_like_dist = stats.entropy(relu_like_hist, base=2)\n",
    "    entropies.append(entropy_relu_like_dist)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(entropies)\n",
    "plt.xlabel(\"Percentage of activations set to 0\")\n",
    "plt.ylabel(\"Entropy of the activation histogram\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _copy_neighbour(to_transform, iterations):\n",
    "    \"\"\"Sets every second value of a vector to the value of the preceding entry\"\"\"\n",
    "    to_transform = np.copy(to_transform)\n",
    "    to_transform_mass = np.sum(to_transform)\n",
    "\n",
    "    #shuffle nad replace all nonzero values to simulate more compact representation\n",
    "    nonzeros = to_transform[to_transform != 0]\n",
    "    zeros = np.zeros((to_transform.shape[0] - nonzeros.shape[0]))\n",
    "\n",
    "    for iteration in range(iterations):\n",
    "        nonzeros[1::2] = nonzeros[0:-1:2]\n",
    "        np.random.shuffle(nonzeros)\n",
    "\n",
    "    transformed = np.concatenate((nonzeros, zeros))\n",
    "    return transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By repeatedly setting the value of every second data sample to its preceding neighbour, we \"simplify\" the representation in the information theoretic sense. After 200 iterations of this transformation, we arrive at a distributions which looks qualitatively similar to those observed after restricting the norm of the weight vector. (See notebook 7.weight_renormalization.ipynb for experiment details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now impose structure on the 1st till 30th bin\n",
    "transformed = _copy_neighbour(relu_like_dist, 200) \n",
    "\n",
    "transformed_hist, _, _ = plt.hist(transformed, bins=30)\n",
    "entropy_transformed_dist = stats.entropy(transformed_hist, base=2)\n",
    "print(f'Entropy of the transformed distribution: {entropy_transformed_dist}')\n",
    "print(f'Compared to the relu like distribution this is a decrease of {entropy_relu_like_dist-entropy_transformed_dist} bits')\n",
    "plt.xlabel(\"Magnitude of activations\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the effect of imposing structure on the non-zero activations reduced the entropy further, but not even matching the amount of redution that was obtained by the relu activation alone. \n",
    "\n",
    "We therefore conclude that the bias of the relu activation function towards the mutual information with the input is strong, especially as the its size is not even matched by artifically imposed structure (for example by restricted weight norm) on the nonzero activations. We furthermore hypothesize that the relu compresses in the sense of the information botteneck framework, but does this immediately and not in a distinct phase of the training process. A more qunatitative approach testing this hypothesis is subject of another notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropies = []\n",
    "for iterations in np.arange(0, 200, 1):\n",
    "    dist = _copy_neighbour(relu_like_dist, iterations)\n",
    "    hist, _ = np.histogram(dist, bins=30)\n",
    "    entropy_dist = stats.entropy(hist, base=2)\n",
    "    entropies.append(entropy_dist)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(entropies)\n",
    "plt.xlabel(\"Number of copy_neighbour iterations\")\n",
    "plt.ylabel(\"Entropy of the activation histogram\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above shows the an example of development of entropy over the iterations of \"simplifying\" the representation. Interesting in this regard is the high variability. The decreasing trand for the value of the entroy with increasing number of iterations is clearly visible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
